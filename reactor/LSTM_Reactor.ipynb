{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_uKU4r2u35E"
      },
      "source": [
        "### FNO Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MmpQEbUpG8Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import h5py\n",
        "import torch.nn as nn\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "import datetime\n",
        "#################################################\n",
        "#\n",
        "# Utilities\n",
        "#\n",
        "#################################################\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# reading data\n",
        "class MatReader(object):\n",
        "    def __init__(self, file_path, to_torch=True, to_cuda=False, to_float=True):\n",
        "        super(MatReader, self).__init__()\n",
        "\n",
        "        self.to_torch = to_torch\n",
        "        self.to_cuda = to_cuda\n",
        "        self.to_float = to_float\n",
        "\n",
        "        self.file_path = file_path\n",
        "\n",
        "        self.data = None\n",
        "        self.old_mat = None\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            self.data = scipy.io.loadmat(self.file_path)\n",
        "            self.old_mat = True\n",
        "        except:\n",
        "            self.data = h5py.File(self.file_path)\n",
        "            self.old_mat = False\n",
        "\n",
        "    def load_file(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self._load_file()\n",
        "\n",
        "    def read_field(self, field):\n",
        "        x = self.data[field]\n",
        "\n",
        "        if not self.old_mat:\n",
        "            x = x[()]\n",
        "            x = np.transpose(x, axes=range(len(x.shape) - 1, -1, -1))\n",
        "\n",
        "        if self.to_float:\n",
        "            x = x.astype(np.float32)\n",
        "\n",
        "        if self.to_torch:\n",
        "            x = torch.from_numpy(x)\n",
        "\n",
        "            if self.to_cuda:\n",
        "                x = x.cuda()\n",
        "\n",
        "        return x\n",
        "\n",
        "    def set_cuda(self, to_cuda):\n",
        "        self.to_cuda = to_cuda\n",
        "\n",
        "    def set_torch(self, to_torch):\n",
        "        self.to_torch = to_torch\n",
        "\n",
        "    def set_float(self, to_float):\n",
        "        self.to_float = to_float\n",
        "\n",
        "# normalization, pointwise gaussian\n",
        "class UnitGaussianNormalizer(object):\n",
        "    def __init__(self, x, eps=0.00001):\n",
        "        super(UnitGaussianNormalizer, self).__init__()\n",
        "\n",
        "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
        "        self.mean = torch.mean(x, 0)\n",
        "        self.std = torch.std(x, 0)\n",
        "        self.eps = eps\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = (x - self.mean) / (self.std + self.eps)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        if sample_idx is None:\n",
        "            std = self.std + self.eps # n\n",
        "            mean = self.mean\n",
        "        else:\n",
        "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
        "                std = self.std[sample_idx] + self.eps  # batch*n\n",
        "                mean = self.mean[sample_idx]\n",
        "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
        "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
        "                mean = self.mean[:,sample_idx]\n",
        "\n",
        "        # x is in shape of batch*n or T*batch*n\n",
        "        x = (x * std) + mean\n",
        "        return x\n",
        "\n",
        "    def cuda(self):\n",
        "        self.mean = self.mean.cuda()\n",
        "        self.std = self.std.cuda()\n",
        "\n",
        "    def cpu(self):\n",
        "        self.mean = self.mean.cpu()\n",
        "        self.std = self.std.cpu()\n",
        "\n",
        "# normalization, Gaussian\n",
        "class GaussianNormalizer(object):\n",
        "    def __init__(self, x, eps=0.00001):\n",
        "        super(GaussianNormalizer, self).__init__()\n",
        "\n",
        "        self.mean = torch.mean(x)\n",
        "        self.std = torch.std(x)\n",
        "        self.eps = eps\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = (x - self.mean) / (self.std + self.eps)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        x = (x * (self.std + self.eps)) + self.mean\n",
        "        return x\n",
        "\n",
        "    def cuda(self):\n",
        "        self.mean = self.mean.cuda()\n",
        "        self.std = self.std.cuda()\n",
        "\n",
        "    def cpu(self):\n",
        "        self.mean = self.mean.cpu()\n",
        "        self.std = self.std.cpu()\n",
        "\n",
        "\n",
        "# normalization, scaling by range\n",
        "class RangeNormalizer(object):\n",
        "    def __init__(self, x, low=0.0, high=1.0):\n",
        "        super(RangeNormalizer, self).__init__()\n",
        "        mymin = torch.min(x, 0)[0].view(-1)\n",
        "        mymax = torch.max(x, 0)[0].view(-1)\n",
        "\n",
        "        self.a = (high - low)/(mymax - mymin)\n",
        "        self.b = -self.a*mymax + high\n",
        "\n",
        "    def encode(self, x):\n",
        "        s = x.size()\n",
        "        x = x.view(s[0], -1)\n",
        "        x = self.a*x + self.b\n",
        "        x = x.view(s)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x):\n",
        "        s = x.size()\n",
        "        x = x.view(s[0], -1)\n",
        "        x = (x - self.b)/self.a\n",
        "        x = x.view(s)\n",
        "        return x\n",
        "\n",
        "#loss function with rel/abs Lp loss\n",
        "class LpLoss(object):\n",
        "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
        "        super(LpLoss, self).__init__()\n",
        "\n",
        "        #Dimension and Lp-norm type are postive\n",
        "        assert d > 0 and p > 0\n",
        "\n",
        "        self.d = d\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def abs(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "\n",
        "        #Assume uniform mesh\n",
        "        h = 1.0 / (x.size()[1] - 1.0)\n",
        "\n",
        "        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)\n",
        "\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(all_norms)\n",
        "            else:\n",
        "                return torch.sum(all_norms)\n",
        "\n",
        "        return all_norms\n",
        "\n",
        "    def rel(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "\n",
        "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
        "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
        "\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(diff_norms/y_norms)\n",
        "            else:\n",
        "                return torch.sum(diff_norms/y_norms)\n",
        "\n",
        "        return diff_norms/y_norms\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        return self.rel(x, y)\n",
        "\n",
        "# Sobolev norm (HS norm)\n",
        "# where we also compare the numerical derivatives between the output and target\n",
        "class HsLoss(object):\n",
        "    def __init__(self, d=2, p=2, k=1, a=None, group=False, size_average=True, reduction=True):\n",
        "        super(HsLoss, self).__init__()\n",
        "\n",
        "        #Dimension and Lp-norm type are postive\n",
        "        assert d > 0 and p > 0\n",
        "\n",
        "        self.d = d\n",
        "        self.p = p\n",
        "        self.k = k\n",
        "        self.balanced = group\n",
        "        self.reduction = reduction\n",
        "        self.size_average = size_average\n",
        "\n",
        "        if a == None:\n",
        "            a = [1,] * k\n",
        "        self.a = a\n",
        "\n",
        "    def rel(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
        "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(diff_norms/y_norms)\n",
        "            else:\n",
        "                return torch.sum(diff_norms/y_norms)\n",
        "        return diff_norms/y_norms\n",
        "\n",
        "    def __call__(self, x, y, a=None):\n",
        "        nx = x.size()[1]\n",
        "        ny = x.size()[2]\n",
        "        k = self.k\n",
        "        balanced = self.balanced\n",
        "        a = self.a\n",
        "        x = x.view(x.shape[0], nx, ny, -1)\n",
        "        y = y.view(y.shape[0], nx, ny, -1)\n",
        "\n",
        "        k_x = torch.cat((torch.arange(start=0, end=nx//2, step=1),torch.arange(start=-nx//2, end=0, step=1)), 0).reshape(nx,1).repeat(1,ny)\n",
        "        k_y = torch.cat((torch.arange(start=0, end=ny//2, step=1),torch.arange(start=-ny//2, end=0, step=1)), 0).reshape(1,ny).repeat(nx,1)\n",
        "        k_x = torch.abs(k_x).reshape(1,nx,ny,1).to(x.device)\n",
        "        k_y = torch.abs(k_y).reshape(1,nx,ny,1).to(x.device)\n",
        "\n",
        "        x = torch.fft.fftn(x, dim=[1, 2])\n",
        "        y = torch.fft.fftn(y, dim=[1, 2])\n",
        "\n",
        "        if balanced==False:\n",
        "            weight = 1\n",
        "            if k >= 1:\n",
        "                weight += a[0]**2 * (k_x**2 + k_y**2)\n",
        "            if k >= 2:\n",
        "                weight += a[1]**2 * (k_x**4 + 2*k_x**2*k_y**2 + k_y**4)\n",
        "            weight = torch.sqrt(weight)\n",
        "            loss = self.rel(x*weight, y*weight)\n",
        "        else:\n",
        "            loss = self.rel(x, y)\n",
        "            if k >= 1:\n",
        "                weight = a[0] * torch.sqrt(k_x**2 + k_y**2)\n",
        "                loss += self.rel(x*weight, y*weight)\n",
        "            if k >= 2:\n",
        "                weight = a[1] * torch.sqrt(k_x**4 + 2*k_x**2*k_y**2 + k_y**4)\n",
        "                loss += self.rel(x*weight, y*weight)\n",
        "            loss = loss / (k+1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# A simple feedforward neural network\n",
        "class DenseNet(torch.nn.Module):\n",
        "    def __init__(self, layers, nonlinearity, out_nonlinearity=None, normalize=False):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        self.n_layers = len(layers) - 1\n",
        "\n",
        "        assert self.n_layers >= 1\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for j in range(self.n_layers):\n",
        "            self.layers.append(nn.Linear(layers[j], layers[j+1]))\n",
        "\n",
        "            if j != self.n_layers - 1:\n",
        "                if normalize:\n",
        "                    self.layers.append(nn.BatchNorm1d(layers[j+1]))\n",
        "\n",
        "                self.layers.append(nonlinearity())\n",
        "\n",
        "        if out_nonlinearity is not None:\n",
        "            self.layers.append(out_nonlinearity())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for _, l in enumerate(self.layers):\n",
        "            x = l(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# print the number of parameters\n",
        "def count_params(model):\n",
        "    c = 0\n",
        "    for p in list(model.parameters()):\n",
        "        c += reduce(operator.mul, \n",
        "                    list(p.size()+(2,) if p.is_complex() else p.size()))\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ6nMxz6pPOE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from typing import List, Optional\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "def adam(params: List[Tensor],\n",
        "         grads: List[Tensor],\n",
        "         exp_avgs: List[Tensor],\n",
        "         exp_avg_sqs: List[Tensor],\n",
        "         max_exp_avg_sqs: List[Tensor],\n",
        "         state_steps: List[int],\n",
        "         *,\n",
        "         amsgrad: bool,\n",
        "         beta1: float,\n",
        "         beta2: float,\n",
        "         lr: float,\n",
        "         weight_decay: float,\n",
        "         eps: float):\n",
        "    r\"\"\"Functional API that performs Adam algorithm computation.\n",
        "    See :class:`~torch.optim.Adam` for details.\n",
        "    \"\"\"\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "\n",
        "        grad = grads[i]\n",
        "        exp_avg = exp_avgs[i]\n",
        "        exp_avg_sq = exp_avg_sqs[i]\n",
        "        step = state_steps[i]\n",
        "\n",
        "        bias_correction1 = 1 - beta1 ** step\n",
        "        bias_correction2 = 1 - beta2 ** step\n",
        "\n",
        "        if weight_decay != 0:\n",
        "            grad = grad.add(param, alpha=weight_decay)\n",
        "\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
        "        if amsgrad:\n",
        "            # Maintains the maximum of all 2nd moment running avg. till now\n",
        "            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
        "            # Use the max. for normalizing running avg. of gradient\n",
        "            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "        else:\n",
        "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "\n",
        "        step_size = lr / bias_correction1\n",
        "\n",
        "        param.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    r\"\"\"Implements Adam algorithm.\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The implementation of the L2 penalty follows changes proposed in\n",
        "    `Decoupled Weight Decay Regularization`_.\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            exp_avg_sqs = []\n",
        "            max_exp_avg_sqs = []\n",
        "            state_steps = []\n",
        "            beta1, beta2 = group['betas']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    params_with_grad.append(p)\n",
        "                    if p.grad.is_sparse:\n",
        "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                    grads.append(p.grad)\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    # Lazy state initialization\n",
        "                    if len(state) == 0:\n",
        "                        state['step'] = 0\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        # Exponential moving average of squared gradient values\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        if group['amsgrad']:\n",
        "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                    exp_avgs.append(state['exp_avg'])\n",
        "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
        "\n",
        "                    if group['amsgrad']:\n",
        "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
        "\n",
        "                    # update the steps for each param group update\n",
        "                    state['step'] += 1\n",
        "                    # record the step after step update\n",
        "                    state_steps.append(state['step'])\n",
        "\n",
        "            adam(params_with_grad,\n",
        "                 grads,\n",
        "                 exp_avgs,\n",
        "                 exp_avg_sqs,\n",
        "                 max_exp_avg_sqs,\n",
        "                 state_steps,\n",
        "                 amsgrad=group['amsgrad'],\n",
        "                 beta1=beta1,\n",
        "                 beta2=beta2,\n",
        "                 lr=group['lr'],\n",
        "                 weight_decay=group['weight_decay'],\n",
        "                 eps=group['eps'])\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tudHVVb8pSFE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@author: Zongyi Li\n",
        "This file is the Fourier Neural Operator for 1D problem such as the (time-independent) Burgers equation \n",
        "discussed in Section 5.1 in the [paper](https://arxiv.org/pdf/2010.08895.pdf).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "from timeit import default_timer\n",
        "# from utilities3 import *\n",
        "\n",
        "# from Adam import Adam\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyQreHqGMhQT"
      },
      "outputs": [],
      "source": [
        "################################################################\n",
        "#  1d fourier layer\n",
        "################################################################\n",
        "class SpectralConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, modes1):\n",
        "        super(SpectralConv1d, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
        "        \"\"\"\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.modes1 = modes1  #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
        "\n",
        "        self.scale = (1 / (in_channels*out_channels))\n",
        "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
        "\n",
        "    # Complex multiplication\n",
        "    def compl_mul1d(self, input, weights):\n",
        "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
        "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.shape[0]\n",
        "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
        "        x_ft = torch.fft.rfft(x)\n",
        "\n",
        "        # Multiply relevant Fourier modes\n",
        "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1)//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
        "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
        "\n",
        "        #Return to physical space\n",
        "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
        "        return x\n",
        "\n",
        "class FNO1d(nn.Module):\n",
        "    def __init__(self, modes, width):\n",
        "        super(FNO1d, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        The overall network. It contains 4 layers of the Fourier layer.\n",
        "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
        "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
        "            W defined by self.w; K defined by self.conv .\n",
        "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
        "        \n",
        "        input: the solution of the initial condition and location (a(x), x)\n",
        "        input shape: (batchsize, x=s, c=2)\n",
        "        output: the solution of a later timestep\n",
        "        output shape: (batchsize, x=s, c=1)\n",
        "        \"\"\"\n",
        "\n",
        "        self.modes1 = modes\n",
        "        self.width = width\n",
        "        self.padding = 9 # pad the domain if input is non-periodic\n",
        "        self.fc0 = nn.Linear(3, self.width) # input channel is 2: (a(x), x)\n",
        "\n",
        "        self.conv0 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.conv1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.conv2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.conv3 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.width, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        grid = self.get_grid(x.shape, x.device)\n",
        "        x = torch.cat((x, grid), dim=-1)\n",
        "        x = self.fc0(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # x = F.pad(x, [0,self.padding]) # pad the domain if input is non-periodic\n",
        "\n",
        "        x1 = self.conv0(x)\n",
        "        x2 = self.w0(x)\n",
        "        x = x1 + x2\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.w1(x)\n",
        "        x = x1 + x2\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x1 = self.conv2(x)\n",
        "        x2 = self.w2(x)\n",
        "        x = x1 + x2\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x1 = self.conv3(x)\n",
        "        x2 = self.w3(x)\n",
        "        x = x1 + x2\n",
        "\n",
        "        # x = x[..., :-self.padding] # pad the domain if input is non-periodic\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def get_grid(self, shape, device):\n",
        "        batchsize, size_x = shape[0], shape[1]\n",
        "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
        "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
        "        return gridx.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjWXKwUIftiu"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYZZn2tzftiv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "\n",
        "from timeit import default_timer\n",
        "# from utilities3 import *\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, layer, width, x_size=512):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.num_layers = layer\n",
        "        self.hidden_size = width\n",
        "        self.x_size = x_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=x_size, hidden_size=width, num_layers=layer)\n",
        "\n",
        "        self.fc = nn.Linear(width, x_size//2)\n",
        "\n",
        "    def forward(self, x, h=None, c=None):\n",
        "\n",
        "        T_size = x.shape[0]\n",
        "        batch_size = x.shape[1]\n",
        "\n",
        "        # h, c (num_layers * num_directions, batch, hidden_size)\n",
        "        if h ==None:\n",
        "            h, c = self.init_hidden(shape=(self.num_layers, batch_size, self.hidden_size), device=x.device)\n",
        "\n",
        "        #input (seq_len, batch, input_size)\n",
        "        out, (h, c) = self.lstm(x.view(T_size, batch_size, self.x_size), (h, c))\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, shape, device):\n",
        "        return (torch.zeros(shape, device=device),\n",
        "                torch.zeros(shape, device=device))\n",
        "\n",
        "    def count_params(self):\n",
        "        c = 0\n",
        "        for p in self.parameters():\n",
        "            c += reduce(operator.mul, list(p.size()))\n",
        "\n",
        "        return c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qRtAOUKVgger"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data"
      ],
      "metadata": {
        "id": "rvCtUlr1ggt8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7nAOKiCftix",
        "outputId": "2616d242-c2d3-4f74-d71c-3d10966629b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "#  Load Data\n",
        "################################################################\n",
        "obs_data = torch.load('/content/drive/My Drive/Colab Notebooks/data/heat_obs_data0218.pt')\n",
        "sys_data = torch.load('/content/drive/My Drive/Colab Notebooks/data/heat_sys_data0218.pt')\n",
        "obs_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azmyE-BAgi6C",
        "outputId": "06c6ea1b-b1f9-45b0-a303-aa8727215423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1200, 5001, 101])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of3ILAeBftix"
      },
      "outputs": [],
      "source": [
        "subsampling = 50\n",
        "obs_data = obs_data[:, ::subsampling, :]\n",
        "sys_data = sys_data[:, ::subsampling, :]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nt = sys_data.shape[1]\n",
        "nx = sys_data.shape[2]\n",
        "\n",
        "print(nt)\n",
        "print(nx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8UL0Xai5Av2",
        "outputId": "66024f23-2a4e-4924-d078-1277873dce70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n",
            "101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w10RiMUyftiy",
        "outputId": "e9df034a-fd2e-442a-d060-4d5af89c3f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial, initial shape torch.Size([1200, 100, 101])\n"
          ]
        }
      ],
      "source": [
        "initial = obs_data[:,:-1:,:]\n",
        "print('initial, initial shape', initial.shape)\n",
        "# initial = initial[:, ::10, :]\n",
        "# initial = torch.repeat_interleave(initial, 10, dim=1)\n",
        "# print('initial shape', initial.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw6lEuGoftiy",
        "outputId": "4041cbc2-2be4-402d-f762-aa3bc6bb3af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial boundary shape torch.Size([1200, 100, 1])\n",
            "boundary shape torch.Size([1200, 100, 101])\n"
          ]
        }
      ],
      "source": [
        "boundary = sys_data[:, 1:, -1:]\n",
        "print('initial boundary shape', boundary.shape)\n",
        "boundary = boundary.repeat(1,1,nx)\n",
        "print('boundary shape', boundary.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iR7fMOWftiz",
        "outputId": "41f0b995-5ee1-4b53-bf24-0cbcf85b4a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1200, 100, 202]) torch.Size([1200, 100, 101])\n"
          ]
        }
      ],
      "source": [
        "# initial = obs_data[:,:-1:,:]\n",
        "# # initial_pturb = initial+ 0.05* torch.rand(initial.shape) \n",
        "\n",
        "# boundary = sys_data[:, 1:, -1:].repeat(1,1,nx) #boundary measurement at the right\n",
        "\n",
        "input = torch.cat([initial, boundary], dim=-1)\n",
        "output = obs_data[:,1:,:]\n",
        "print(input.shape, output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "nlN9Mvwf66YI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2yoerBIftiw",
        "outputId": "dd17d74c-f278-4cc3-a6ee-d7a4ace19ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer, width, batch_size, learning_rate, epochs 1 1000 20 0.001 200\n"
          ]
        }
      ],
      "source": [
        "Ntrain = 1000 # training instances\n",
        "Ntest = 200 # testing instances\n",
        "\n",
        "nt = input.shape[1] \n",
        "nx = output.shape[2] \n",
        "\n",
        "t = 10\n",
        "T_iter = nt//t\n",
        "\n",
        "ntrain = Ntrain * T_iter * t\n",
        "ntest = Ntest * T_iter * t\n",
        "\n",
        "s =  nx\n",
        "\n",
        "batch_size = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "epochs = 200\n",
        "step_size = 10\n",
        "gamma = 0.5\n",
        "ep_print = 1\n",
        "\n",
        "layer = 1\n",
        "width = 1000\n",
        "\n",
        "print('layer, width, batch_size, learning_rate, epochs', layer, width, batch_size, learning_rate, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASWb9QRdftiz",
        "outputId": "e692c526-33e1-486a-f8af-9a3317baeaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 10, 10, 202])\n",
            "torch.Size([1000, 10, 10, 101])\n"
          ]
        }
      ],
      "source": [
        "x_train = input[:Ntrain, :, :].reshape((Ntrain, T_iter, t, nx*2))\n",
        "y_train = output[:Ntrain, :, :].reshape((Ntrain, T_iter, t, nx))\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "x_test = input[-Ntest:, :, :].reshape((Ntest, T_iter, t, nx*2))\n",
        "y_test = output[-Ntest:, :, :].reshape((Ntest, T_iter, t, nx))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEyr_F2yftiz",
        "outputId": "a6d0afff-9071-4246-c0b8-52f24ac262bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4917101\n"
          ]
        }
      ],
      "source": [
        "model = LSTM(layer=layer, width=width, x_size=nx*2).cuda()\n",
        "\n",
        "print(model.count_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "yGgU1mS_ftiz",
        "outputId": "10e42ca9-b5b8-4b81-8fe8-02ef95cfc0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.563030550999997 0.7134118862533569 0.8597345314025879 0.5785524787902832 0.7732077074050904\n",
            "1 2.4446555729999773 0.5518864921188354 0.7509247741699219 0.5485629358291626 0.7391623878479003\n",
            "2 2.449090872000056 0.5129185518264772 0.7173122358322144 0.4946664346694946 0.6966812086105346\n",
            "3 2.4464897760000213 0.48894553600311275 0.6915366878509521 0.4897465686798096 0.6961387252807617\n",
            "4 2.446656648000044 0.4722554892730713 0.6786333589553833 0.4679067226409912 0.6726330232620239\n",
            "5 2.4459364099999448 0.479601750869751 0.6828749237060547 0.46675243549346923 0.6762368297576904\n",
            "6 2.4859193480000386 0.4668307600402832 0.6681671371459961 0.4544430501937866 0.6569189310073853\n",
            "7 2.5125912629999902 0.45240166847229 0.651326174736023 0.4536450826644898 0.658213963508606\n",
            "8 2.4472752089999403 0.45246119943618773 0.6549319276809692 0.4666043882369995 0.6565949964523315\n",
            "9 2.4610824210000146 0.4506555634307861 0.6494614953994751 0.4458822639465332 0.6431608676910401\n",
            "10 2.4502929090000407 0.43404716224670403 0.6356837701797485 0.43252770137786867 0.6340345335006714\n",
            "11 2.4531825679999884 0.42059716007232667 0.6241329278945923 0.4154694097518921 0.6185599994659424\n",
            "12 2.452733282000054 0.40543363599777227 0.6079398488998413 0.4008332449913025 0.6032549571990967\n",
            "13 2.449049741000067 0.41147099222183225 0.6131909399032592 0.41066663665771486 0.6197747087478638\n",
            "14 2.4499033949999784 0.4065497542762756 0.6230080423355102 0.3978062385559082 0.6131981754302979\n",
            "15 2.4489164820000724 0.3914365333557129 0.6064075508117676 0.3858373006820679 0.6007584047317505\n",
            "16 2.4579708659999824 0.3875826296329498 0.5988520221710205 0.3828687460899353 0.5919791698455811\n",
            "17 2.4511166730000014 0.40398887563705443 0.6071047000885009 0.4015556552886963 0.613828010559082\n",
            "18 2.4527416230000654 0.4148530770874023 0.6280344705581665 0.40260275468826295 0.622501106262207\n",
            "19 2.4479445080000914 0.4201734989929199 0.634943748474121 0.426608380317688 0.6473567914962769\n",
            "20 2.4477835310000273 0.42015688183784483 0.6489003801345825 0.4175978517532349 0.6500348377227784\n",
            "21 2.4481681160000335 0.4124162124919891 0.6414931488037109 0.4101231830120087 0.639355411529541\n",
            "22 2.4557156839999834 0.4023535057735443 0.6276449575424194 0.40258879232406614 0.62453293800354\n",
            "23 2.4506174100000635 0.392122739982605 0.6177510242462159 0.39870301418304444 0.6172744941711426\n",
            "24 2.4553478480000877 0.3867153747558594 0.610882963180542 0.40560345478057863 0.6040974140167237\n",
            "25 2.45163474900005 0.3838888937664032 0.6026231021881103 0.38697320456504825 0.6112222337722778\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d1c2e8bcb68c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtrain_traj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mtrain_l2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "########### training\n",
        "myloss = LpLoss(size_average=False)\n",
        "# y_normalizer.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "error = np.zeros((epochs+1, 2))\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    t1 = default_timer()\n",
        "    train_l2 = 0\n",
        "    train_overall = 0\n",
        "    train_traj = np.zeros(T_iter, )\n",
        "\n",
        "    for xx, yy in train_loader:\n",
        "        xx = xx.to(device)\n",
        "        yy = yy.to(device)\n",
        "        h = None\n",
        "        c = None\n",
        "        y_pred = torch.zeros_like(yy).to(device)\n",
        "\n",
        "        for i in range(0, T_iter):\n",
        "\n",
        "            # xx,yy: (batch, T_iter, t, s)\n",
        "            x = xx[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "            y = yy[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "\n",
        "            im, h, c = model(x, h, c)\n",
        "\n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "\n",
        "            y_pred[:, i, :, :] = im.permute(1,0,2)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss = myloss(im.reshape(-1, s), y.reshape(-1, s))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_traj[i] += loss.item()\n",
        "            train_l2 += loss.item()\n",
        "                    \n",
        "        #mse = F.mse_loss(y_pred.reshape(batch_size, -1), yy.reshape(batch_size, -1), reduction='mean')\n",
        "        #train_mse += mse.item()\n",
        "        loss_overall = myloss(y_pred.reshape(batch_size, -1, s), yy.reshape(batch_size, -1, s))\n",
        "        train_overall += loss_overall.item()\n",
        "        \n",
        "\n",
        "    if ep % ep_print == ep_print-1:\n",
        "        test_l2 = 0\n",
        "        test_traj = np.zeros(T_iter, )\n",
        "        test_overall = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xx, yy in test_loader:\n",
        "                xx = xx.to(device)\n",
        "                yy = yy.to(device)\n",
        "                h = None\n",
        "                c = None\n",
        "                for i in range(0, T_iter):\n",
        "                    # xx,yy: (batch, T_iter, t, s)\n",
        "                    x = xx[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "                    y = yy[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "\n",
        "                    im, h, c = model(x, h, c)\n",
        "                    y_pred[:, i, :, :] = im.permute(1,0,2)\n",
        "\n",
        "                    loss = myloss(im.reshape(-1, s), y.reshape(-1, s))\n",
        "                    test_traj[i] += loss.item()\n",
        "                    test_l2 += loss.item()\n",
        "                loss_overall = myloss(y_pred.reshape(batch_size, -1, s), yy.reshape(batch_size, -1, s))\n",
        "                test_overall += loss_overall.item()\n",
        "        \n",
        "\n",
        "        t2 = default_timer()\n",
        "        \n",
        "        train_l2 = train_l2 / Ntrain /T_iter / t\n",
        "        train_traj = train_traj / Ntrain / t\n",
        "        train_overall = train_overall  / Ntrain\n",
        "        test_l2 = test_l2 / (T_iter*Ntest*t)\n",
        "        test_traj = test_traj / (Ntest*t)\n",
        "        test_overall = test_overall/Ntest\n",
        "        # train_mse /= Ntrain\n",
        "\n",
        "        print(ep, t2 - t1, train_l2, train_overall, test_l2, test_overall)\n",
        "        # print(ep, t2 - t1, train_l2, train_traj, test_l2, test_traj)\n",
        "        error[ep] = [train_l2, train_overall]\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCagLSirfti0",
        "outputId": "93e8e717-c6b1-47a6-e548-30f5eb0021b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 296,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#torch.save(model.state_dict(), 'model/lstm_reactor')\n",
        "model1 = LSTM(layer=layer, width=width, x_size=nx*2).cuda()\n",
        "model1.load_state_dict(torch.load('model/lstm_reactor'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_in = 0\n",
        "T_out = 50\n",
        "T_warmup = 15\n",
        "T_iter = (T_out-T_in)\n",
        "print(T_out, T_warmup)\n",
        "\n",
        "# dataloader = MatReader(PATH_DATA)\n",
        "# x_test = dataloader.read_field('u')[-1, T_in, ::sub].reshape(1, s)\n",
        "# y_test = dataloader.read_field('u')[-1, T_in:T_out, ::sub].reshape(T_iter, s)\n",
        "\n",
        "x_test = x_test.reshape((Ntest, nt, 2*nx))\n",
        "y_test = y_test.reshape((Ntest, nt, nx))\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w8g2JtR8FkA",
        "outputId": "3da28d92-4f7c-45b5-884c-e0f936cd1ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 15\n",
            "torch.Size([200, 50, 202])\n",
            "torch.Size([200, 50, 101])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAYFIPPfti0",
        "outputId": "00de3be9-519c-45a1-8c20-f03e95ae3638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5955810564011336 0.02376598708500069\n"
          ]
        }
      ],
      "source": [
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "ep_print = 1\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = torch.zeros(Ntest, T_iter, s)\n",
        "    pred = pred.cuda()\n",
        "    errors = torch.zeros(Ntest, T_iter, )\n",
        "    error_overall = 0\n",
        "    index = 0\n",
        "    h = None\n",
        "    c = None\n",
        "    \n",
        "    #no warm up\n",
        "    t1 = default_timer()\n",
        "    for x, y in test_loader:\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        x_in = x[:, 0:1, :]\n",
        "        pred[index, 0, :] = x_in[:, :, 0:nx].reshape(s)\n",
        "        \n",
        "        for t in range(1, T_iter):\n",
        "          out, h, c = model(x_in, h, c)\n",
        "          pred[index, t, :] = out.reshape(s)\n",
        "\n",
        "          x_in = x[:, t+1:t+2, :]\n",
        "          if(t>T_warmup):\n",
        "            x_in[:, :, 0:nx] = out.reshape(1, 1, s)\n",
        "\n",
        "          l2 = myloss(out.view(1, -1), y[:,t,:].view(1, -1)).item()\n",
        "          errors[index, t] = l2\n",
        "        \n",
        "        total_l2 = myloss(pred[index, T_warmup:].view(1, -1), y[:, T_warmup:].view(1, -1)).item()\n",
        "        error_overall += total_l2\n",
        "        \n",
        "        # if index % ep_print == ep_print-1:\n",
        "        #     print(index, l2)\n",
        "        index = index + 1\n",
        "    t2 = default_timer()\n",
        "\n",
        "    print(error_overall/Ntest, (t2-t1)/Ntest)\n",
        "\n",
        "#     #warm up\n",
        "#     pred2 = torch.zeros(T_iter, s)\n",
        "#     errors2 = torch.zeros(T_iter, )\n",
        "#     index = 0\n",
        "#     out = x_test.cuda()\n",
        "#     h = None\n",
        "#     c = None\n",
        "#     for y, in test_loader:\n",
        "#         x_in = out.view(1, 1, s)\n",
        "#         y = y.cuda()\n",
        "\n",
        "#         out, h, c = model(x_in, h, c)\n",
        "#         pred2[index] = out.reshape(1,s)\n",
        "\n",
        "#         l2 = myloss(out.view(1, -1), y.view(1, -1)).item()\n",
        "#         errors2[index] = l2\n",
        "#         if index % ep_print == ep_print-1:\n",
        "#             print(index, l2)\n",
        "#         index = index + 1\n",
        "\n",
        "#         if index+T_in < T_warmup:\n",
        "#             out = y\n",
        "\n",
        "# scipy.io.savemat(path_pred, mdict={'pred': pred.cpu().numpy(), 'pred2': pred2.cpu().numpy(), 'y': y_test.cpu().numpy(),})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "GB_JRvYdfti1",
        "outputId": "43b73fca-4371-4fcb-c918-51d8ee93c44d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa773501350>]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(errors[:, T_warmup:].mean(dim=0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "LSTM_Reactor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}