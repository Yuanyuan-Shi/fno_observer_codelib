{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_uKU4r2u35E"
      },
      "source": [
        "### FNO Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MmpQEbUpG8Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import h5py\n",
        "import torch.nn as nn\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "import datetime\n",
        "#################################################\n",
        "#\n",
        "# Utilities\n",
        "#\n",
        "#################################################\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# reading data\n",
        "class MatReader(object):\n",
        "    def __init__(self, file_path, to_torch=True, to_cuda=False, to_float=True):\n",
        "        super(MatReader, self).__init__()\n",
        "\n",
        "        self.to_torch = to_torch\n",
        "        self.to_cuda = to_cuda\n",
        "        self.to_float = to_float\n",
        "\n",
        "        self.file_path = file_path\n",
        "\n",
        "        self.data = None\n",
        "        self.old_mat = None\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            self.data = scipy.io.loadmat(self.file_path)\n",
        "            self.old_mat = True\n",
        "        except:\n",
        "            self.data = h5py.File(self.file_path)\n",
        "            self.old_mat = False\n",
        "\n",
        "    def load_file(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self._load_file()\n",
        "\n",
        "    def read_field(self, field):\n",
        "        x = self.data[field]\n",
        "\n",
        "        if not self.old_mat:\n",
        "            x = x[()]\n",
        "            x = np.transpose(x, axes=range(len(x.shape) - 1, -1, -1))\n",
        "\n",
        "        if self.to_float:\n",
        "            x = x.astype(np.float32)\n",
        "\n",
        "        if self.to_torch:\n",
        "            x = torch.from_numpy(x)\n",
        "\n",
        "            if self.to_cuda:\n",
        "                x = x.cuda()\n",
        "\n",
        "        return x\n",
        "\n",
        "    def set_cuda(self, to_cuda):\n",
        "        self.to_cuda = to_cuda\n",
        "\n",
        "    def set_torch(self, to_torch):\n",
        "        self.to_torch = to_torch\n",
        "\n",
        "    def set_float(self, to_float):\n",
        "        self.to_float = to_float\n",
        "\n",
        "# normalization, pointwise gaussian\n",
        "class UnitGaussianNormalizer(object):\n",
        "    def __init__(self, x, eps=0.00001):\n",
        "        super(UnitGaussianNormalizer, self).__init__()\n",
        "\n",
        "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
        "        self.mean = torch.mean(x, 0)\n",
        "        self.std = torch.std(x, 0)\n",
        "        self.eps = eps\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = (x - self.mean) / (self.std + self.eps)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        if sample_idx is None:\n",
        "            std = self.std + self.eps # n\n",
        "            mean = self.mean\n",
        "        else:\n",
        "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
        "                std = self.std[sample_idx] + self.eps  # batch*n\n",
        "                mean = self.mean[sample_idx]\n",
        "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
        "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
        "                mean = self.mean[:,sample_idx]\n",
        "\n",
        "        # x is in shape of batch*n or T*batch*n\n",
        "        x = (x * std) + mean\n",
        "        return x\n",
        "\n",
        "    def cuda(self):\n",
        "        self.mean = self.mean.cuda()\n",
        "        self.std = self.std.cuda()\n",
        "\n",
        "    def cpu(self):\n",
        "        self.mean = self.mean.cpu()\n",
        "        self.std = self.std.cpu()\n",
        "\n",
        "# normalization, Gaussian\n",
        "class GaussianNormalizer(object):\n",
        "    def __init__(self, x, eps=0.00001):\n",
        "        super(GaussianNormalizer, self).__init__()\n",
        "\n",
        "        self.mean = torch.mean(x)\n",
        "        self.std = torch.std(x)\n",
        "        self.eps = eps\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = (x - self.mean) / (self.std + self.eps)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        x = (x * (self.std + self.eps)) + self.mean\n",
        "        return x\n",
        "\n",
        "    def cuda(self):\n",
        "        self.mean = self.mean.cuda()\n",
        "        self.std = self.std.cuda()\n",
        "\n",
        "    def cpu(self):\n",
        "        self.mean = self.mean.cpu()\n",
        "        self.std = self.std.cpu()\n",
        "\n",
        "\n",
        "# normalization, scaling by range\n",
        "class RangeNormalizer(object):\n",
        "    def __init__(self, x, low=0.0, high=1.0):\n",
        "        super(RangeNormalizer, self).__init__()\n",
        "        mymin = torch.min(x, 0)[0].view(-1)\n",
        "        mymax = torch.max(x, 0)[0].view(-1)\n",
        "\n",
        "        self.a = (high - low)/(mymax - mymin)\n",
        "        self.b = -self.a*mymax + high\n",
        "\n",
        "    def encode(self, x):\n",
        "        s = x.size()\n",
        "        x = x.view(s[0], -1)\n",
        "        x = self.a*x + self.b\n",
        "        x = x.view(s)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x):\n",
        "        s = x.size()\n",
        "        x = x.view(s[0], -1)\n",
        "        x = (x - self.b)/self.a\n",
        "        x = x.view(s)\n",
        "        return x\n",
        "\n",
        "#loss function with rel/abs Lp loss\n",
        "class LpLoss(object):\n",
        "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
        "        super(LpLoss, self).__init__()\n",
        "\n",
        "        #Dimension and Lp-norm type are postive\n",
        "        assert d > 0 and p > 0\n",
        "\n",
        "        self.d = d\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def abs(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "\n",
        "        #Assume uniform mesh\n",
        "        h = 1.0 / (x.size()[1] - 1.0)\n",
        "\n",
        "        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)\n",
        "\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(all_norms)\n",
        "            else:\n",
        "                return torch.sum(all_norms)\n",
        "\n",
        "        return all_norms\n",
        "\n",
        "    def rel(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "\n",
        "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
        "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
        "\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(diff_norms/y_norms)\n",
        "            else:\n",
        "                return torch.sum(diff_norms/y_norms)\n",
        "\n",
        "        return diff_norms/y_norms\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        return self.rel(x, y)\n",
        "\n",
        "# Sobolev norm (HS norm)\n",
        "# where we also compare the numerical derivatives between the output and target\n",
        "class HsLoss(object):\n",
        "    def __init__(self, d=2, p=2, k=1, a=None, group=False, size_average=True, reduction=True):\n",
        "        super(HsLoss, self).__init__()\n",
        "\n",
        "        #Dimension and Lp-norm type are postive\n",
        "        assert d > 0 and p > 0\n",
        "\n",
        "        self.d = d\n",
        "        self.p = p\n",
        "        self.k = k\n",
        "        self.balanced = group\n",
        "        self.reduction = reduction\n",
        "        self.size_average = size_average\n",
        "\n",
        "        if a == None:\n",
        "            a = [1,] * k\n",
        "        self.a = a\n",
        "\n",
        "    def rel(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
        "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(diff_norms/y_norms)\n",
        "            else:\n",
        "                return torch.sum(diff_norms/y_norms)\n",
        "        return diff_norms/y_norms\n",
        "\n",
        "    def __call__(self, x, y, a=None):\n",
        "        nx = x.size()[1]\n",
        "        ny = x.size()[2]\n",
        "        k = self.k\n",
        "        balanced = self.balanced\n",
        "        a = self.a\n",
        "        x = x.view(x.shape[0], nx, ny, -1)\n",
        "        y = y.view(y.shape[0], nx, ny, -1)\n",
        "\n",
        "        k_x = torch.cat((torch.arange(start=0, end=nx//2, step=1),torch.arange(start=-nx//2, end=0, step=1)), 0).reshape(nx,1).repeat(1,ny)\n",
        "        k_y = torch.cat((torch.arange(start=0, end=ny//2, step=1),torch.arange(start=-ny//2, end=0, step=1)), 0).reshape(1,ny).repeat(nx,1)\n",
        "        k_x = torch.abs(k_x).reshape(1,nx,ny,1).to(x.device)\n",
        "        k_y = torch.abs(k_y).reshape(1,nx,ny,1).to(x.device)\n",
        "\n",
        "        x = torch.fft.fftn(x, dim=[1, 2])\n",
        "        y = torch.fft.fftn(y, dim=[1, 2])\n",
        "\n",
        "        if balanced==False:\n",
        "            weight = 1\n",
        "            if k >= 1:\n",
        "                weight += a[0]**2 * (k_x**2 + k_y**2)\n",
        "            if k >= 2:\n",
        "                weight += a[1]**2 * (k_x**4 + 2*k_x**2*k_y**2 + k_y**4)\n",
        "            weight = torch.sqrt(weight)\n",
        "            loss = self.rel(x*weight, y*weight)\n",
        "        else:\n",
        "            loss = self.rel(x, y)\n",
        "            if k >= 1:\n",
        "                weight = a[0] * torch.sqrt(k_x**2 + k_y**2)\n",
        "                loss += self.rel(x*weight, y*weight)\n",
        "            if k >= 2:\n",
        "                weight = a[1] * torch.sqrt(k_x**4 + 2*k_x**2*k_y**2 + k_y**4)\n",
        "                loss += self.rel(x*weight, y*weight)\n",
        "            loss = loss / (k+1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# A simple feedforward neural network\n",
        "class DenseNet(torch.nn.Module):\n",
        "    def __init__(self, layers, nonlinearity, out_nonlinearity=None, normalize=False):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        self.n_layers = len(layers) - 1\n",
        "\n",
        "        assert self.n_layers >= 1\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for j in range(self.n_layers):\n",
        "            self.layers.append(nn.Linear(layers[j], layers[j+1]))\n",
        "\n",
        "            if j != self.n_layers - 1:\n",
        "                if normalize:\n",
        "                    self.layers.append(nn.BatchNorm1d(layers[j+1]))\n",
        "\n",
        "                self.layers.append(nonlinearity())\n",
        "\n",
        "        if out_nonlinearity is not None:\n",
        "            self.layers.append(out_nonlinearity())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for _, l in enumerate(self.layers):\n",
        "            x = l(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# print the number of parameters\n",
        "def count_params(model):\n",
        "    c = 0\n",
        "    for p in list(model.parameters()):\n",
        "        c += reduce(operator.mul, \n",
        "                    list(p.size()+(2,) if p.is_complex() else p.size()))\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ6nMxz6pPOE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from typing import List, Optional\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "def adam(params: List[Tensor],\n",
        "         grads: List[Tensor],\n",
        "         exp_avgs: List[Tensor],\n",
        "         exp_avg_sqs: List[Tensor],\n",
        "         max_exp_avg_sqs: List[Tensor],\n",
        "         state_steps: List[int],\n",
        "         *,\n",
        "         amsgrad: bool,\n",
        "         beta1: float,\n",
        "         beta2: float,\n",
        "         lr: float,\n",
        "         weight_decay: float,\n",
        "         eps: float):\n",
        "    r\"\"\"Functional API that performs Adam algorithm computation.\n",
        "    See :class:`~torch.optim.Adam` for details.\n",
        "    \"\"\"\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "\n",
        "        grad = grads[i]\n",
        "        exp_avg = exp_avgs[i]\n",
        "        exp_avg_sq = exp_avg_sqs[i]\n",
        "        step = state_steps[i]\n",
        "\n",
        "        bias_correction1 = 1 - beta1 ** step\n",
        "        bias_correction2 = 1 - beta2 ** step\n",
        "\n",
        "        if weight_decay != 0:\n",
        "            grad = grad.add(param, alpha=weight_decay)\n",
        "\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
        "        if amsgrad:\n",
        "            # Maintains the maximum of all 2nd moment running avg. till now\n",
        "            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
        "            # Use the max. for normalizing running avg. of gradient\n",
        "            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "        else:\n",
        "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "\n",
        "        step_size = lr / bias_correction1\n",
        "\n",
        "        param.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    r\"\"\"Implements Adam algorithm.\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The implementation of the L2 penalty follows changes proposed in\n",
        "    `Decoupled Weight Decay Regularization`_.\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            exp_avg_sqs = []\n",
        "            max_exp_avg_sqs = []\n",
        "            state_steps = []\n",
        "            beta1, beta2 = group['betas']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    params_with_grad.append(p)\n",
        "                    if p.grad.is_sparse:\n",
        "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                    grads.append(p.grad)\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    # Lazy state initialization\n",
        "                    if len(state) == 0:\n",
        "                        state['step'] = 0\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        # Exponential moving average of squared gradient values\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        if group['amsgrad']:\n",
        "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                    exp_avgs.append(state['exp_avg'])\n",
        "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
        "\n",
        "                    if group['amsgrad']:\n",
        "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
        "\n",
        "                    # update the steps for each param group update\n",
        "                    state['step'] += 1\n",
        "                    # record the step after step update\n",
        "                    state_steps.append(state['step'])\n",
        "\n",
        "            adam(params_with_grad,\n",
        "                 grads,\n",
        "                 exp_avgs,\n",
        "                 exp_avg_sqs,\n",
        "                 max_exp_avg_sqs,\n",
        "                 state_steps,\n",
        "                 amsgrad=group['amsgrad'],\n",
        "                 beta1=beta1,\n",
        "                 beta2=beta2,\n",
        "                 lr=group['lr'],\n",
        "                 weight_decay=group['weight_decay'],\n",
        "                 eps=group['eps'])\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tudHVVb8pSFE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@author: Zongyi Li\n",
        "This file is the Fourier Neural Operator for 1D problem such as the (time-independent) Burgers equation \n",
        "discussed in Section 5.1 in the [paper](https://arxiv.org/pdf/2010.08895.pdf).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "from timeit import default_timer\n",
        "# from utilities3 import *\n",
        "\n",
        "# from Adam import Adam\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyQreHqGMhQT"
      },
      "outputs": [],
      "source": [
        "################################################################\n",
        "#  1d fourier layer\n",
        "################################################################\n",
        "class SpectralConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, modes1):\n",
        "        super(SpectralConv1d, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
        "        \"\"\"\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.modes1 = modes1  #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
        "\n",
        "        self.scale = (1 / (in_channels*out_channels))\n",
        "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
        "\n",
        "    # Complex multiplication\n",
        "    def compl_mul1d(self, input, weights):\n",
        "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
        "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.shape[0]\n",
        "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
        "        x_ft = torch.fft.rfft(x)\n",
        "\n",
        "        # Multiply relevant Fourier modes\n",
        "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1)//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
        "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
        "\n",
        "        #Return to physical space\n",
        "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
        "        return x\n",
        "\n",
        "class FNO1d(nn.Module):\n",
        "    def __init__(self, modes, width):\n",
        "        super(FNO1d, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        The overall network. It contains 4 layers of the Fourier layer.\n",
        "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
        "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
        "            W defined by self.w; K defined by self.conv .\n",
        "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
        "        \n",
        "        input: the solution of the initial condition and location (a(x), x)\n",
        "        input shape: (batchsize, x=s, c=2)\n",
        "        output: the solution of a later timestep\n",
        "        output shape: (batchsize, x=s, c=1)\n",
        "        \"\"\"\n",
        "\n",
        "        self.modes1 = modes\n",
        "        self.width = width\n",
        "        self.padding = 9 # pad the domain if input is non-periodic\n",
        "        self.fc0 = nn.Linear(3, self.width) # input channel is 2: (a(x), x)\n",
        "\n",
        "        self.conv0 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.conv1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.conv2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.conv3 = SpectralConv1d(self.width, self.width, self.modes1)\n",
        "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.width, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        grid = self.get_grid(x.shape, x.device)\n",
        "        x = torch.cat((x, grid), dim=-1)\n",
        "        x = self.fc0(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # x = F.pad(x, [0,self.padding]) # pad the domain if input is non-periodic\n",
        "\n",
        "        x1 = self.conv0(x)\n",
        "        x2 = self.w0(x)\n",
        "        x = x1 + x2\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.w1(x)\n",
        "        x = x1 + x2\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x1 = self.conv2(x)\n",
        "        x2 = self.w2(x)\n",
        "        x = x1 + x2\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x1 = self.conv3(x)\n",
        "        x2 = self.w3(x)\n",
        "        x = x1 + x2\n",
        "\n",
        "        # x = x[..., :-self.padding] # pad the domain if input is non-periodic\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def get_grid(self, shape, device):\n",
        "        batchsize, size_x = shape[0], shape[1]\n",
        "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
        "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
        "        return gridx.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjWXKwUIftiu"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYZZn2tzftiv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "\n",
        "from timeit import default_timer\n",
        "# from utilities3 import *\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, layer, width, x_size=512):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.num_layers = layer\n",
        "        self.hidden_size = width\n",
        "        self.x_size = x_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=x_size, hidden_size=width, num_layers=layer)\n",
        "\n",
        "        self.fc = nn.Linear(width, x_size//3)\n",
        "\n",
        "    def forward(self, x, h=None, c=None):\n",
        "\n",
        "        T_size = x.shape[0]\n",
        "        batch_size = x.shape[1]\n",
        "\n",
        "        # h, c (num_layers * num_directions, batch, hidden_size)\n",
        "        if h ==None:\n",
        "            h, c = self.init_hidden(shape=(self.num_layers, batch_size, self.hidden_size), device=x.device)\n",
        "\n",
        "        #input (seq_len, batch, input_size)\n",
        "        out, (h, c) = self.lstm(x.view(T_size, batch_size, self.x_size), (h, c))\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, shape, device):\n",
        "        return (torch.zeros(shape, device=device),\n",
        "                torch.zeros(shape, device=device))\n",
        "\n",
        "    def count_params(self):\n",
        "        c = 0\n",
        "        for p in self.parameters():\n",
        "            c += reduce(operator.mul, list(p.size()))\n",
        "\n",
        "        return c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qRtAOUKVgger"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data"
      ],
      "metadata": {
        "id": "rvCtUlr1ggt8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7nAOKiCftix",
        "outputId": "a63c6310-4de2-4883-8a47-575a16c9b630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# configs\n",
        "################################################################\n",
        "sys_data_PATH = '/content/drive/My Drive/Colab Notebooks/data/ARZ_sys_data.mat'\n",
        "obs_data_PATH = '/content/drive/My Drive/Colab Notebooks/data/ARZ_obs_data.mat'"
      ],
      "metadata": {
        "id": "azmyE-BAgi6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_reader = MatReader(sys_data_PATH)\n",
        "sys_data = sys_reader.read_field('sys_data')\n",
        "\n",
        "obs_reader = MatReader(obs_data_PATH)\n",
        "obs_data = obs_reader.read_field('obs_data')"
      ],
      "metadata": {
        "id": "zD_2o9EzF0es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nt = 1201\n",
        "nx = 101"
      ],
      "metadata": {
        "id": "KX1eTIR9F-aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of3ILAeBftix"
      },
      "outputs": [],
      "source": [
        "# data = torch.cat([sys_data, obs_data], dim=-1)[:,:,::10,:]\n",
        "subsampling = 40\n",
        "tt = nt//subsampling\n",
        "sys_data = sys_data[:,:,::subsampling,:]\n",
        "obs_data = obs_data[:,:,::subsampling,:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys_data[:,:,:, 0] = sys_data[:,:,:, 0]*10\n",
        "sys_data[:,:,:, 1] = sys_data[:,:,:, 1]/10\n",
        "\n",
        "obs_data[:,:,:, 0] = obs_data[:,:,:, 0]*10\n",
        "obs_data[:,:,:, 1] = obs_data[:,:,:, 1]/10"
      ],
      "metadata": {
        "id": "B8UL0Xai5Av2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w10RiMUyftiy",
        "outputId": "405b8f5c-fa98-4618-f95f-e313e409612b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1200, 30, 606]) torch.Size([1200, 30, 202])\n"
          ]
        }
      ],
      "source": [
        "initial = obs_data[:,:,:-1,:]\n",
        "# initial_pturb = initial+ 0.05* torch.rand(initial.shape) \n",
        "\n",
        "boundary_right = sys_data[:, -1:, 1:,:].repeat(1,nx,1,1) #right boundary\n",
        "boundary_left = sys_data[:, 0:1, 1:, :].repeat(1,nx,1,1)\n",
        "boundary = torch.cat([boundary_right, boundary_left], dim=-1)\n",
        "\n",
        "input = torch.cat([initial, boundary], dim=-1)\n",
        "output = obs_data[:,:,1:,:]\n",
        "\n",
        "input = input.permute(0,2,1,3).reshape(1200, tt, nx*6)\n",
        "output = output.permute(0,2,1,3).reshape(1200, tt, nx*2)\n",
        "\n",
        "print(input.shape, output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nt = input.shape[1]\n",
        "nx = input.shape[2]//6\n",
        "\n",
        "print(nt)\n",
        "print(nx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8rPZZF8LfqB",
        "outputId": "b6bed609-45bd-4c67-e812-1d56291b458a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ss1_w3VJLuor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "nlN9Mvwf66YI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2yoerBIftiw",
        "outputId": "cb01dcc5-6da7-4280-a130-1de425806b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer, width, batch_size, learning_rate, epochs 1 1000 20 0.001 200\n"
          ]
        }
      ],
      "source": [
        "Ntrain = 1000 # training instances\n",
        "Ntest = 200 # testing instances\n",
        "\n",
        "\n",
        "t = 10\n",
        "T_iter = nt//t\n",
        "\n",
        "ntrain = Ntrain * T_iter * t\n",
        "ntest = Ntest * T_iter * t\n",
        "\n",
        "s =  nx\n",
        "\n",
        "batch_size = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "epochs = 200\n",
        "step_size = 10\n",
        "gamma = 0.5\n",
        "ep_print = 1\n",
        "\n",
        "layer = 1\n",
        "width = 1000\n",
        "\n",
        "print('layer, width, batch_size, learning_rate, epochs', layer, width, batch_size, learning_rate, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASWb9QRdftiz",
        "outputId": "b1c82b57-dfc7-475c-da34-ff4d48dc2e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 3, 10, 606])\n",
            "torch.Size([1000, 3, 10, 202])\n"
          ]
        }
      ],
      "source": [
        "x_train = input[:Ntrain, :, :].reshape((Ntrain, T_iter, t, nx*6))\n",
        "y_train = output[:Ntrain, :, :].reshape((Ntrain, T_iter, t, nx*2))\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "x_test = input[-Ntest:, :, :].reshape((Ntest, T_iter, t, nx*6))\n",
        "y_test = output[-Ntest:, :, :].reshape((Ntest, T_iter, t, nx*2))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEyr_F2yftiz",
        "outputId": "49384f16-f363-4de2-f995-d07f3e7cc483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6634202\n"
          ]
        }
      ],
      "source": [
        "model = LSTM(layer=layer, width=width, x_size=nx*6).cuda()\n",
        "\n",
        "print(model.count_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGgU1mS_ftiz",
        "outputId": "8004eb04-f6f5-4b23-bed0-12c688804ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.0011011850001523 0.1570204092025757 0.0838820899128914 0.10428844006856283 0.05477332770824432\n",
            "1 0.9134415980001904 0.08716564404169719 0.04599543106555939 0.07355814441045125 0.039306895434856416\n",
            "2 0.909070843000336 0.06933987469673157 0.03660728573799133 0.06001617765426636 0.03194528639316559\n",
            "3 0.9121583269998155 0.05932859528859456 0.031434821486473086 0.04699135216077169 0.025771297067403793\n",
            "4 0.9127780029998576 0.054063186041514076 0.028720591127872466 0.05341254496574402 0.02871636688709259\n",
            "5 0.9094312830002309 0.05314211451212565 0.028299212247133256 0.043718954960505166 0.02395098775625229\n",
            "6 0.988322104999952 0.048425649038950594 0.025786966979503632 0.0427462674776713 0.023347375988960264\n",
            "7 0.9624308209999981 0.04729306279818217 0.02520779711008072 0.049610810756683346 0.026352255940437316\n",
            "8 0.9137283769996429 0.046281587902704875 0.024648896396160126 0.04769458039601644 0.0251530197262764\n",
            "9 0.9145693779996691 0.0457094851175944 0.024364255130290984 0.03807373921076457 0.020814141929149627\n",
            "10 0.9105827680000402 0.037183141597112015 0.020177117824554443 0.0329360560576121 0.01852779358625412\n",
            "11 0.9093944879996343 0.035767793798446655 0.01941206121444702 0.032190879940986634 0.018155333250761033\n",
            "12 0.9140859819999605 0.03545436922709147 0.019220768183469773 0.0307772114276886 0.017485733181238174\n",
            "13 0.9156891960001303 0.03432774345080058 0.018654966324567795 0.03254752826690674 0.01807108774781227\n",
            "14 0.911601568000151 0.03406557728449504 0.01850085759162903 0.03090899344285329 0.0173496975004673\n",
            "15 0.9126564449998114 0.03342050854365031 0.018140352874994277 0.029840526421864827 0.016857538968324662\n",
            "16 0.9093308460001026 0.032646913242340085 0.017754702121019362 0.030015971978505453 0.0168241448700428\n",
            "17 0.9111545330001718 0.031918662937482196 0.017397928535938263 0.032562651952107745 0.017785264402627943\n",
            "18 0.9114016949997676 0.031473282011349996 0.01716150751709938 0.028090595046679177 0.015909963548183442\n",
            "19 0.9089614869999423 0.03132392106850942 0.017069834500551224 0.0291655322710673 0.016260499060153963\n",
            "20 0.9114678149999236 0.02798898296356201 0.015580226480960847 0.025221192399660745 0.014631164371967315\n",
            "21 0.9118951629998264 0.026683406209945675 0.014994972586631775 0.024605215628941853 0.01432779610157013\n",
            "22 0.9089820620001774 0.026568949524561562 0.014893371373414994 0.024089260935783386 0.014072654992341995\n",
            "23 0.9112145930002953 0.026484830904006957 0.014817482382059098 0.023730913480122885 0.013908631056547166\n",
            "24 0.9100587140001153 0.025845153959592183 0.014511932522058487 0.024080394824345906 0.013935839235782623\n",
            "25 0.9120194329998412 0.025964012900988258 0.014514032423496246 0.024731162230173746 0.014282682090997695\n",
            "26 0.9140105780002159 0.025648104763031004 0.014348066180944443 0.025749488353729248 0.014504192918539048\n",
            "27 0.9115489420000813 0.025321786586443584 0.014207724273204804 0.024159775018692018 0.013882930278778077\n",
            "28 0.9204847619998873 0.024802939295768735 0.013918977826833726 0.022428252498308817 0.013142551109194756\n",
            "29 0.9160239429998001 0.02521971371968587 0.014089705616235734 0.024174121499061584 0.013755210489034653\n",
            "30 0.9139120379995802 0.02393215623696645 0.013519736766815185 0.021589667359987894 0.012752885445952416\n",
            "31 0.9159909800000605 0.023305611300468446 0.013217348292469979 0.021673680067062377 0.012740507870912552\n",
            "32 0.911741161000009 0.02342754951318105 0.013230563223361969 0.021000399589538574 0.012493923977017403\n",
            "33 0.910088715999791 0.02263401133219401 0.01291079181432724 0.020999022682507834 0.0124680195748806\n",
            "34 0.9102256829996804 0.022174747546513877 0.012700508683919906 0.020965469161669414 0.01240855373442173\n",
            "35 0.9114210950001507 0.02198921473821004 0.01264508107304573 0.02092231031258901 0.012374948188662529\n",
            "36 0.9133178949996363 0.0218471807718277 0.012547347813844681 0.020640088240305582 0.012260867059230804\n",
            "37 0.9118480649999583 0.02178660855293274 0.012494717568159104 0.02013283429543177 0.012047122791409492\n",
            "38 0.910207421999985 0.02159612433115641 0.01239807364344597 0.02026977757612864 0.012079940810799598\n",
            "39 0.9134100669998588 0.021245339393615722 0.012286037623882294 0.020093645334243775 0.011981334984302521\n",
            "40 0.9069800790002773 0.020734914183616636 0.0120310590416193 0.019624216318130494 0.011793822348117829\n",
            "41 0.9167308040000535 0.020226448051134747 0.011838026642799378 0.01948920214176178 0.011730498895049095\n",
            "42 0.9119558820002567 0.020009301654497783 0.01175210103392601 0.019385941942532858 0.01167934738099575\n",
            "43 0.9121793129997968 0.01988184490203857 0.011692593902349472 0.019284548819065096 0.011627732738852502\n",
            "44 0.9121145029998843 0.019791513196627298 0.011644914656877517 0.019191320459047954 0.011579101011157035\n",
            "45 0.9111788469999738 0.01969834037621816 0.011598437890410424 0.01910669684410095 0.011534635648131371\n",
            "46 0.9133374390003155 0.019781147813796994 0.011604124575853348 0.01902229901154836 0.011490389704704285\n",
            "47 0.9118325869999353 0.019635207001368206 0.011543288573622704 0.018931626617908478 0.011443792134523392\n",
            "48 0.9107013670000015 0.019653164529800416 0.011532820910215378 0.018867709398269653 0.011407136023044585\n",
            "49 0.9111148969996066 0.019563671286900838 0.011485764294862748 0.018820989787578582 0.011379857063293457\n",
            "50 0.9104069969998818 0.018522210812568666 0.011173108026385308 0.018682590524355572 0.011323480680584908\n",
            "51 0.9126914739999847 0.01844318978389104 0.01113932716846466 0.018629953881104788 0.0113004370033741\n",
            "52 0.9112870120002299 0.018395171344280245 0.01111497400701046 0.018572964350382488 0.011275370195508004\n",
            "53 0.9119540169999709 0.01834940430323283 0.01109039182960987 0.01851626455783844 0.011247964277863502\n",
            "54 0.9107755410000209 0.018300295209884642 0.011063589990139008 0.01846578707297643 0.01122191607952118\n",
            "55 0.9112989180002842 0.018278079986572267 0.011043639391660691 0.01837514054775238 0.011179566383361816\n",
            "56 0.9124460599996382 0.01831860782702764 0.011039459452033043 0.018323537270228066 0.01115211732685566\n",
            "57 1.0129580950001582 0.018381896682580313 0.01104328815639019 0.018327985604604084 0.011146841868758202\n",
            "58 0.9746740870000394 0.018260390329360963 0.010996694728732109 0.018215205510457355 0.011096856892108917\n",
            "59 0.9107856820000961 0.0183521778066953 0.011010097205638886 0.018173214038213092 0.011073906719684602\n",
            "60 0.912862959999984 0.01792202006578445 0.010883686512708664 0.01811135592063268 0.011045545041561126\n",
            "61 0.9114563279999857 0.01787785458564758 0.010864768773317338 0.018081912239392598 0.011031112149357796\n",
            "62 0.9113009650000095 0.017847454377015435 0.010849411904811859 0.018053024590015412 0.011017428860068321\n",
            "63 0.9129074400002537 0.01781443528731664 0.010832972481846809 0.018025573968887328 0.011003567948937415\n",
            "64 0.9101292700001977 0.017782667450110114 0.01081688529253006 0.017994193573792776 0.010988309681415558\n",
            "65 0.9113873579999563 0.017750206828117367 0.01080060711503029 0.017954361120859783 0.010970291271805764\n",
            "66 0.9104542750001201 0.017719473930199937 0.010784436672925949 0.017928076366583508 0.010956878736615182\n",
            "67 0.9097911880003267 0.017694934388001758 0.010769385695457458 0.01790147308508555 0.010942490920424462\n",
            "68 0.9113352120002673 0.017671542894840242 0.010754540905356408 0.01784311491250992 0.010916599780321121\n",
            "69 0.9467186189999666 0.017659557044506072 0.010742167398333549 0.017801944971084595 0.010896964967250823\n",
            "70 0.9457477819996711 0.017544807902971905 0.010705178305506706 0.017748524963855743 0.010873398259282113\n",
            "71 0.9495577520001461 0.017518727028369905 0.010693442538380622 0.017731005668640137 0.01086384803056717\n",
            "72 0.9118300579998504 0.017498448781172436 0.010683595314621925 0.017711825887362163 0.010854046046733856\n",
            "73 0.9116525560002628 0.017478591040770212 0.010673724427819251 0.017690390149752298 0.010843687653541566\n",
            "74 0.9127821090000907 0.017457705346743266 0.01066335079073906 0.017667068719863892 0.010832969173789024\n",
            "75 0.9106798660000095 0.01743534698486328 0.010652398660779 0.01764591815074285 0.010822923704981804\n",
            "76 0.9116684649998206 0.01741281901995341 0.01064131972193718 0.017627237439155577 0.010813375636935234\n",
            "77 0.9157651979999173 0.017391068557898202 0.010630410388112068 0.017608305811882018 0.01080357350409031\n",
            "78 0.9128887560000294 0.0173693753639857 0.010619482561945915 0.017585787653923036 0.010792399197816849\n",
            "79 0.9107519700000921 0.017346858282883963 0.010608293741941452 0.017560456057389578 0.010780166238546371\n",
            "80 0.9091646840001886 0.017304806530475617 0.01059192369878292 0.017527847985426583 0.01076811045408249\n",
            "81 0.9116189390001637 0.017292070599397025 0.010585677489638328 0.017516129910945893 0.010762461423873902\n",
            "82 0.911608926000099 0.017280702022711435 0.010579943254590034 0.01750378676255544 0.010756466537714005\n",
            "83 0.9100518159998501 0.017269112567106882 0.010574113339185716 0.017490757703781126 0.010750142112374305\n",
            "84 0.9105757559996164 0.017257200805346172 0.010568160206079483 0.01747733328739802 0.010743625909090043\n",
            "85 0.9108373270000811 0.017244983287652334 0.010562093779444694 0.017463780264059703 0.01073703460395336\n",
            "86 0.911982622999858 0.01723252022266388 0.01055593729019165 0.017450228611628215 0.010730426833033562\n",
            "87 0.9087918420000278 0.017219869693120317 0.010549712657928466 0.017436728417873384 0.010723825693130493\n",
            "88 0.909173964999809 0.01720707056125005 0.010543436020612716 0.01742330539226532 0.01071723960340023\n",
            "89 0.9116469340001458 0.017194150726000466 0.010537119626998902 0.017409979939460755 0.010710676014423371\n",
            "90 0.9130090019998534 0.017167180355389913 0.01052687320113182 0.017400575598080954 0.010707321166992188\n",
            "91 0.9090824470004009 0.017160202674070993 0.010523493573069573 0.01739334817727407 0.010703889280557632\n",
            "92 0.9118091919999642 0.017153593623638153 0.010520235598087312 0.017386133233706157 0.010700441375374794\n",
            "93 0.910014788999888 0.01714694581826528 0.010516953587532043 0.017378891030947367 0.01069696456193924\n",
            "94 0.9106364340000255 0.01714023191134135 0.010513637512922287 0.01737161314487457 0.010693459138274193\n",
            "95 0.9115522960000817 0.01713344243764877 0.010510285213589668 0.017364299734433493 0.010689929351210594\n",
            "96 0.9109588209998947 0.017126574643452963 0.01050689832866192 0.017356955269972482 0.010686379075050354\n",
            "97 0.9091682070002207 0.01711963169972102 0.01050347925722599 0.017349586427211763 0.010682812482118607\n",
            "98 0.9119174409997868 0.017112620389461517 0.010500032678246498 0.01734219960371653 0.010679233893752098\n",
            "99 0.9136063220003052 0.017105549454689027 0.010496562823653221 0.017334800918896994 0.010675646886229516\n",
            "100 0.9101093350000156 0.01709178163607915 0.010491221249103545 0.017329885760943096 0.010674452632665634\n",
            "101 0.9106290879999506 0.017087788625558216 0.010489405125379563 0.017326059619585672 0.010672630444169044\n",
            "102 0.9120900729999448 0.017084075625737508 0.010487602025270461 0.017322218398253122 0.010670781582593918\n",
            "103 0.9075943039997583 0.01708034656047821 0.010485786512494088 0.01731832234064738 0.010668903365731239\n",
            "104 0.9079816220000794 0.017076589234670002 0.010483957424759865 0.017314379274845124 0.010667001232504844\n",
            "105 0.912005336999755 0.01707280378739039 0.010482115358114243 0.017310398578643797 0.010665080100297928\n",
            "106 0.9124720400000115 0.017068993039925894 0.01048026143014431 0.01730639092127482 0.01066314622759819\n",
            "107 0.9133704000000762 0.017065158748626706 0.010478396773338318 0.017302366733551025 0.010661202818155288\n",
            "108 0.9100726940000641 0.017061305165290832 0.010476522848010063 0.01729833306868871 0.010659253224730492\n",
            "109 0.9120515199997499 0.017057434988021848 0.01047464106976986 0.017294293502966563 0.010657300800085067\n",
            "110 0.9110201919997962 0.017050281123320264 0.010471746206283569 0.017289756814638773 0.010655362829566003\n",
            "111 0.9131430279999222 0.01704811270634333 0.01047075542807579 0.01728786849975586 0.010654485523700715\n",
            "112 0.9112618220001423 0.017046164270242057 0.010469822108745575 0.01728598310550054 0.010653588697314262\n",
            "113 0.9138772090000202 0.017044204576810197 0.010468874871730805 0.01728406778971354 0.010652668625116348\n",
            "114 0.9122044819996518 0.017042228988806408 0.010467915907502174 0.01728212296962738 0.010651731714606285\n",
            "115 0.9131444690001445 0.017040238650639854 0.010466948047280312 0.017280154446760815 0.010650780647993088\n",
            "116 0.9116137339997294 0.017038235247135162 0.010465973243117332 0.017278165717919666 0.010649819150567055\n",
            "117 0.9098981489996731 0.017036221969127654 0.010464992940425872 0.01727615863084793 0.01064884826540947\n",
            "118 0.9114015290001589 0.017034200020631156 0.010464008331298829 0.01727414043744405 0.010647871121764184\n",
            "119 0.9103947780004091 0.017032171591122948 0.010463020324707032 0.017272110144297283 0.010646888464689254\n",
            "120 0.9106046609999794 0.017027990428606667 0.01046150590479374 0.01726632183790207 0.010644008815288543\n",
            "121 0.910704096000245 0.017026789673169455 0.01046092276275158 0.017265257557233175 0.010643514990806579\n",
            "122 0.9129906590001156 0.017025765895843505 0.010460429668426514 0.017264203409353893 0.01064302034676075\n",
            "123 0.9094147119999434 0.017024737099806467 0.010459932535886764 0.017263147632280984 0.010642520859837532\n",
            "124 0.9097524519997933 0.017023701838652296 0.010459431052207946 0.017262088894844055 0.01064201757311821\n",
            "125 0.9098331299996971 0.01702266073226929 0.010458925798535346 0.017261024753252665 0.010641509890556336\n",
            "126 0.9130708870002309 0.01702161416610082 0.010458417356014252 0.017259957611560822 0.010640999674797058\n",
            "127 0.9124788590002026 0.017020563018321992 0.010457906559109689 0.017258886098861694 0.010640486404299735\n",
            "128 0.9086460799999259 0.01701950802008311 0.010457393318414688 0.017257812281449635 0.01063997194170952\n",
            "129 0.9094475070000954 0.017018449648221336 0.01045687846839428 0.017256736298402152 0.010639454871416092\n",
            "130 0.9100315759997102 0.017016522332032522 0.010456117331981658 0.017255450228850045 0.010638907179236412\n",
            "131 0.9078045629998996 0.017015924076239267 0.010455834537744521 0.01725488595167796 0.010638638064265252\n",
            "132 0.9095952600000601 0.017015384960174563 0.01045557226240635 0.01725433051586151 0.010638372749090194\n",
            "133 0.9111189179998291 0.017014846205711365 0.01045531114935875 0.017253777901331584 0.01063810870051384\n",
            "134 0.9129429969998455 0.01701430668433507 0.010455049633979797 0.0172532266775767 0.010637844651937485\n",
            "135 0.9105806489997121 0.017013766038417817 0.010454787969589234 0.017252676129341126 0.010637580305337907\n",
            "136 0.9131015220000336 0.01701322464545568 0.010454525262117386 0.01725212476650874 0.010637315586209298\n",
            "137 0.9139464889999545 0.017012681913375856 0.010454262137413024 0.01725157384077708 0.010637050196528434\n",
            "138 0.9128104989999883 0.01701213837067286 0.010453998431563378 0.017251021881898244 0.010636784061789513\n",
            "139 0.9144733159996576 0.017011594184239705 0.010453733950853347 0.017250470638275145 0.01063651829957962\n",
            "140 0.9100489630000084 0.01701062628030777 0.010453283950686456 0.01724988422791163 0.010636424794793129\n",
            "141 0.9120305040000858 0.017010311814149224 0.010453161656856537 0.017249594430128735 0.010636288225650787\n",
            "142 0.911511075999897 0.017010037541389467 0.010453028902411461 0.017249310115973155 0.010636151805520057\n",
            "143 0.9101607580000746 0.017009763117631273 0.010452895417809487 0.017249028027057647 0.01063601553440094\n",
            "144 0.913044047999847 0.01700948827266693 0.010452761813998223 0.017248745878537496 0.010635879412293435\n",
            "145 0.9106979009998213 0.01700921361049016 0.010452628344297409 0.01724846464395523 0.010635744109749793\n",
            "146 0.9134529620000649 0.01700893816550573 0.010452494710683822 0.01724818366765976 0.010635608434677124\n",
            "147 0.9102330760001678 0.01700866240262985 0.010452360853552818 0.017247903009255727 0.01063547320663929\n",
            "148 0.9137820249998185 0.017008386989434562 0.010452227160334587 0.017247622946898144 0.010635337978601455\n",
            "149 0.9128055149999454 0.017008111059665678 0.010452093303203583 0.01724734252691269 0.010635202527046203\n",
            "150 0.9094872209998357 0.01700750293334325 0.010451792195439338 0.017247032086054485 0.010635221675038338\n",
            "151 0.9124760689996947 0.017007333791255953 0.010451739460229873 0.017246881465117138 0.010635158643126488\n",
            "152 0.9102239750000081 0.017007193907101946 0.010451674059033394 0.017246737758318583 0.010635091587901116\n",
            "153 0.9083318049997615 0.017007055509090423 0.010451607406139374 0.01724659647544225 0.01063502423465252\n",
            "154 0.9109758429999602 0.01700691653092702 0.010451540246605873 0.01724645407994588 0.010634956210851669\n",
            "155 0.9117522920000738 0.017006778264045716 0.010451473236083985 0.01724631198247274 0.01063488781452179\n",
            "156 0.9119572549998338 0.01700663961172104 0.010451406106352807 0.017246170659859974 0.010634819939732552\n",
            "157 0.910285986999952 0.01700650080045064 0.010451338708400726 0.017246028502782187 0.01063475176692009\n",
            "158 0.9115578989999449 0.0170063619017601 0.010451271295547486 0.017245887438456217 0.010634683892130853\n",
            "159 0.909654416000194 0.017006222860018412 0.01045120406150818 0.017245745221773783 0.010634615942835808\n",
            "160 0.9119608289997814 0.01700589725971222 0.010451030910015107 0.017245619475841523 0.010634634047746658\n",
            "161 0.9107991309997487 0.017005811655521393 0.010451011210680008 0.017245540420214336 0.010634608790278435\n",
            "162 0.9122905839999476 0.017005740976333616 0.010450980335474013 0.0172454687555631 0.01063457876443863\n",
            "163 0.9082791690002523 0.017005672168731688 0.010450948357582092 0.017245398064454395 0.010634546801447868\n",
            "164 0.9112157579997984 0.01700560375849406 0.01045091576874256 0.01724532922108968 0.010634514093399048\n",
            "165 0.9109599350003919 0.017005535674095153 0.010450883209705353 0.01724525874853134 0.010634481236338615\n",
            "166 0.9109604870000112 0.017005466977755228 0.010450850442051888 0.017245189050833384 0.010634448155760765\n",
            "167 0.9107075739998436 0.01700539861917496 0.01045081776380539 0.01724511973063151 0.010634415447711945\n",
            "168 0.9092791920002128 0.017005330216884613 0.0104507846981287 0.01724504949649175 0.010634382143616677\n",
            "169 0.9108394689997112 0.017005261842409768 0.010450751960277557 0.0172449804743131 0.010634349137544631\n",
            "170 0.9099924940001074 0.017005094651381173 0.010450657859444618 0.0172449392080307 0.010634362027049064\n",
            "171 0.9106877280000845 0.01700505649248759 0.010450652882456779 0.01724490064382553 0.01063435360789299\n",
            "172 0.9110606130002452 0.01700502290725708 0.010450640901923179 0.017244865318139395 0.010634341165423394\n",
            "173 0.9132337789997109 0.01700499057372411 0.010450627252459526 0.01724483221769333 0.01063432812690735\n",
            "174 0.9101445040000726 0.017004958701133728 0.010450613260269165 0.01724479939540227 0.010634314119815826\n",
            "175 0.9095170720001988 0.017004926983515422 0.010450598895549773 0.01724476691087087 0.010634299740195275\n",
            "176 0.9114474760003759 0.017004895055294036 0.01045058436691761 0.017244734307130177 0.010634285286068917\n",
            "177 0.910569048999605 0.01700486356417338 0.01045056976377964 0.017244701961676278 0.010634270757436752\n",
            "178 0.9105126590002328 0.017004831945896148 0.01045055516064167 0.01724466953674952 0.010634255930781364\n",
            "179 0.9147647280001365 0.017004800279935204 0.010450540348887443 0.01724463705221812 0.010634240955114364\n",
            "180 0.912202576999789 0.017004712212085722 0.010450490176677703 0.0172446245153745 0.01063424438238144\n",
            "181 0.9084241360001215 0.017004698248704275 0.010450488656759262 0.017244610965251924 0.010634242817759513\n",
            "182 0.91044497699977 0.01700468510389328 0.010450485303997994 0.017244597454865774 0.01063423916697502\n",
            "183 0.9136225290003495 0.01700467246770859 0.010450480848550797 0.017244583209355673 0.010634234175086021\n",
            "184 0.9102893640001639 0.01700465966463089 0.01045047615468502 0.017244569857915244 0.010634229183197022\n",
            "185 0.9097492529999727 0.0170046471118927 0.010450471207499505 0.017244556486606596 0.010634223967790603\n",
            "186 0.9087329710000631 0.01700463476975759 0.010450466081500054 0.01724454241991043 0.01063421830534935\n",
            "187 0.9102142179999646 0.01700462259054184 0.010450461149215698 0.01724452942609787 0.010634213015437126\n",
            "188 0.9076652120002109 0.017004610272248585 0.010450455978512765 0.017244516948858896 0.010634207651019096\n",
            "189 0.9122041319997152 0.01700459792613983 0.010450450628995895 0.01724450300137202 0.010634201914072036\n",
            "190 0.9106510299998263 0.017004557526111606 0.010450427919626236 0.01724449994166692 0.010634202063083648\n",
            "191 0.9116185439997935 0.017004553655783336 0.010450427442789078 0.0172444971203804 0.010634201690554618\n",
            "192 0.91094416299984 0.017004549940427147 0.010450426772236824 0.01724449360370636 0.010634201243519784\n",
            "193 0.9105130519997147 0.01700454635222753 0.010450425997376441 0.017244489908218384 0.010634200349450111\n",
            "194 0.9114277530002255 0.017004542736212415 0.010450424909591674 0.017244485676288605 0.010634199306368828\n",
            "195 0.9102808000002369 0.017004539052645366 0.01045042383670807 0.017244481523831685 0.010634197592735291\n",
            "196 0.9123600590000933 0.017004535249869026 0.010450422525405885 0.017244477927684784 0.010634196326136588\n",
            "197 0.9122737739999138 0.017004531486829122 0.010450421288609504 0.01724447379509608 0.010634194910526275\n",
            "198 0.9094965570002387 0.017004528065522516 0.010450420051813125 0.017244469861189524 0.010634193643927575\n",
            "199 0.9096736670003338 0.01700452445348104 0.010450418710708618 0.017244465907414755 0.010634192377328872\n"
          ]
        }
      ],
      "source": [
        "########### training\n",
        "myloss = LpLoss(size_average=False)\n",
        "# y_normalizer.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "error = np.zeros((epochs+1, 2))\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    t1 = default_timer()\n",
        "    train_l2 = 0\n",
        "    train_overall = 0\n",
        "    train_traj = np.zeros(T_iter, )\n",
        "\n",
        "    for xx, yy in train_loader:\n",
        "        xx = xx.to(device)\n",
        "        yy = yy.to(device)\n",
        "        h = None\n",
        "        c = None\n",
        "        y_pred = torch.zeros_like(yy).to(device)\n",
        "\n",
        "        for i in range(0, T_iter):\n",
        "\n",
        "            # xx,yy: (batch, T_iter, t, s)\n",
        "            x = xx[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "            y = yy[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "\n",
        "            im, h, c = model(x, h, c)\n",
        "\n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "\n",
        "            y_pred[:, i, :, :] = im.permute(1,0,2)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss = myloss(im.reshape(-1, s), y.reshape(-1, s))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_traj[i] += loss.item()\n",
        "            train_l2 += loss.item()\n",
        "                    \n",
        "        #mse = F.mse_loss(y_pred.reshape(batch_size, -1), yy.reshape(batch_size, -1), reduction='mean')\n",
        "        #train_mse += mse.item()\n",
        "        loss_overall = myloss(y_pred.reshape(batch_size, -1, s), yy.reshape(batch_size, -1, s))\n",
        "        train_overall += loss_overall.item()\n",
        "        \n",
        "\n",
        "    if ep % ep_print == ep_print-1:\n",
        "        test_l2 = 0\n",
        "        test_traj = np.zeros(T_iter, )\n",
        "        test_overall = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xx, yy in test_loader:\n",
        "                xx = xx.to(device)\n",
        "                yy = yy.to(device)\n",
        "                h = None\n",
        "                c = None\n",
        "                for i in range(0, T_iter):\n",
        "                    # xx,yy: (batch, T_iter, t, s)\n",
        "                    x = xx[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "                    y = yy[:, i, :, :].permute(1,0,2) # (t, batch, s)\n",
        "\n",
        "                    im, h, c = model(x, h, c)\n",
        "                    y_pred[:, i, :, :] = im.permute(1,0,2)\n",
        "\n",
        "                    loss = myloss(im.reshape(-1, s), y.reshape(-1, s))\n",
        "                    test_traj[i] += loss.item()\n",
        "                    test_l2 += loss.item()\n",
        "                loss_overall = myloss(y_pred.reshape(batch_size, -1, s), yy.reshape(batch_size, -1, s))\n",
        "                test_overall += loss_overall.item()\n",
        "        \n",
        "\n",
        "        t2 = default_timer()\n",
        "        \n",
        "        train_l2 = train_l2 / Ntrain /T_iter / t\n",
        "        train_traj = train_traj / Ntrain / t\n",
        "        train_overall = train_overall  / Ntrain\n",
        "        test_l2 = test_l2 / (T_iter*Ntest*t)\n",
        "        test_traj = test_traj / (Ntest*t)\n",
        "        test_overall = test_overall/Ntest\n",
        "        # train_mse /= Ntrain\n",
        "\n",
        "        print(ep, t2 - t1, train_l2, train_overall, test_l2, test_overall)\n",
        "        # print(ep, t2 - t1, train_l2, train_traj, test_l2, test_traj)\n",
        "        error[ep] = [train_l2, train_overall]\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCagLSirfti0",
        "outputId": "93e8e717-c6b1-47a6-e548-30f5eb0021b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 296,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# #torch.save(model.state_dict(), 'model/lstm_reactor')\n",
        "# model1 = LSTM(layer=layer, width=width, x_size=nx*2).cuda()\n",
        "# model1.load_state_dict(torch.load('model/lstm_reactor'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84PVXjeJM5S7",
        "outputId": "56840b58-d889-44d5-d486-dc0b9e8615f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_in = 0\n",
        "T_out = 30\n",
        "T_warmup = 10\n",
        "T_iter = (T_out-T_in)\n",
        "print(T_out, T_warmup)\n",
        "\n",
        "# dataloader = MatReader(PATH_DATA)\n",
        "# x_test = dataloader.read_field('u')[-1, T_in, ::sub].reshape(1, s)\n",
        "# y_test = dataloader.read_field('u')[-1, T_in:T_out, ::sub].reshape(T_iter, s)\n",
        "\n",
        "x_test = x_test.reshape((Ntest, nt, 6*nx))\n",
        "y_test = y_test.reshape((Ntest, nt, 2*nx))\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "s = 2*nx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w8g2JtR8FkA",
        "outputId": "eb6dcd00-0854-4b4b-c73b-c14d851a849f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 10\n",
            "torch.Size([200, 30, 606])\n",
            "torch.Size([200, 30, 202])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAYFIPPfti0",
        "outputId": "d534c8e9-80f4-440b-907b-ce53c5fc174f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.017884039534255863 0.014202542769999127\n"
          ]
        }
      ],
      "source": [
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "ep_print = 1\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = torch.zeros(Ntest, T_iter, s)\n",
        "    pred = pred.cuda()\n",
        "    errors = torch.zeros(Ntest, T_iter, )\n",
        "    error_overall = 0\n",
        "    index = 0\n",
        "    h = None\n",
        "    c = None\n",
        "    \n",
        "    #no warm up\n",
        "    t1 = default_timer()\n",
        "    for x, y in test_loader:\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        x_in = x[:, 0:1, :]\n",
        "        pred[index, 0, :] = x_in[:, :, 0:2*nx].reshape(s)\n",
        "        \n",
        "        for t in range(1, T_iter):\n",
        "          out, h, c = model(x_in, h, c)\n",
        "          pred[index, t, :] = out.reshape(s)\n",
        "\n",
        "          x_in = x[:, t+1:t+2, :]\n",
        "          if(t>T_warmup):\n",
        "            x_in[:, :, 0:2*nx] = out.reshape(1, 1, s)\n",
        "\n",
        "          l2 = myloss(out.view(1, -1), y[:,t,:].view(1, -1)).item()\n",
        "          errors[index, t] = l2\n",
        "        \n",
        "        total_l2 = myloss(pred[index, T_warmup:].view(1, -1), y[:, T_warmup:].view(1, -1)).item()\n",
        "        error_overall += total_l2\n",
        "        \n",
        "        # if index % ep_print == ep_print-1:\n",
        "        #     print(index, l2)\n",
        "        index = index + 1\n",
        "    t2 = default_timer()\n",
        "\n",
        "    print(error_overall/Ntest, (t2-t1)/Ntest)\n",
        "\n",
        "#     #warm up\n",
        "#     pred2 = torch.zeros(T_iter, s)\n",
        "#     errors2 = torch.zeros(T_iter, )\n",
        "#     index = 0\n",
        "#     out = x_test.cuda()\n",
        "#     h = None\n",
        "#     c = None\n",
        "#     for y, in test_loader:\n",
        "#         x_in = out.view(1, 1, s)\n",
        "#         y = y.cuda()\n",
        "\n",
        "#         out, h, c = model(x_in, h, c)\n",
        "#         pred2[index] = out.reshape(1,s)\n",
        "\n",
        "#         l2 = myloss(out.view(1, -1), y.view(1, -1)).item()\n",
        "#         errors2[index] = l2\n",
        "#         if index % ep_print == ep_print-1:\n",
        "#             print(index, l2)\n",
        "#         index = index + 1\n",
        "\n",
        "#         if index+T_in < T_warmup:\n",
        "#             out = y\n",
        "\n",
        "# scipy.io.savemat(path_pred, mdict={'pred': pred.cpu().numpy(), 'pred2': pred2.cpu().numpy(), 'y': y_test.cpu().numpy(),})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "GB_JRvYdfti1",
        "outputId": "43b73fca-4371-4fcb-c918-51d8ee93c44d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa773501350>]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(errors[:, T_warmup:].mean(dim=0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "LSTM_Traffic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}